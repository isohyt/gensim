{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec to wikipedia articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We conduct the similar experiment to **Document Embedding with Paragraph Vectors** (http://arxiv.org/abs/1507.07998).\n",
    "In this paper, they showed only DBOW results to Wikipedia data. So we replicate this experiments using not only DBOW but also DM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import Doc2Vec module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from gensim.corpora.wikicorpus import WikiCorpus\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, download the dump of all Wikipedia articles from [here](http://download.wikimedia.org/enwiki/) (you want the file enwiki-latest-pages-articles.xml.bz2, or enwiki-YYYYMMDD-pages-articles.xml.bz2 for date-specific dumps).\n",
    "\n",
    "Second, convert the articles to WikiCorpus. WikiCorpus construct a corpus from a Wikipedia (or other MediaWiki-based) database dump.\n",
    "\n",
    "For more details on WikiCorpus, you should access [Corpus from a Wikipedia dump](https://radimrehurek.com/gensim/corpora/wikicorpus.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#wiki = WikiCorpus(\"enwiki-latest-pages-articles.xml.bz2\")\n",
    "#wiki = WikiCorpus(\"enwiki-YYYYMMDD-pages-articles.xml.bz2\")\n",
    "#wiki.save(\"enwikicorpus\")\n",
    "wiki = WikiCorpus.load(\"enwikicorpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define **TaggedWikiDocument** class to convert WikiCorpus into suitable form for Doc2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TaggedWikiDocument(object):\n",
    "    def __init__(self, wiki):\n",
    "        self.wiki = wiki\n",
    "        self.wiki.metadata = True\n",
    "    def __iter__(self):\n",
    "        for content, (page_id, title) in self.wiki.get_texts():\n",
    "            yield TaggedDocument([c.decode(\"utf-8\") for c in content], [title])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = TaggedWikiDocument(wiki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Doc2Vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, we define some types of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "vocab =  915715 # along with paper\n",
    "\n",
    "models = [\n",
    "    # PV-DBOW \n",
    "    Doc2Vec(dm=0, dbow_words=1, size=200, window=5, min_count=5, max_vocab_size=vocab, workers=cores),\n",
    "    # PV-DM w/concatenation\n",
    "    Doc2Vec(dm=1, dm_concat=1, size=200, window=5, min_count=5, max_vocab_size=vocab, workers=cores),\n",
    "    # PV-DM w/average\n",
    "    Doc2Vec(dm=1, dm_mean=1, size=200, window=5, min_count=5, max_vocab_size=vocab, workers=cores),\n",
    "]\n",
    "models[0].build_vocab(documents)\n",
    "print(str(models[0]))\n",
    "for model in models[1:]:\n",
    "    model.reset_from(models[0])\n",
    "    print(str(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now weâ€™re ready to train Doc2Vec of the English Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha, min_alpha, passes = 0.025, 0.001, 10\n",
    "alpha_delta = (alpha - min_alpha) / passes\n",
    "\n",
    "for model in models:\n",
    "    print(str(model))\n",
    "    model.alpha, model.min_alpha = alpha, min_alpha\n",
    "    for epoch in range(passes):\n",
    "        model.train(sentences)\n",
    "        alpha -= alpha_delta\n",
    "    model.save('{}'.format(str(model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, calculating cosine simillarity of **\"Machine learning\"** using Paragraph Vector. Word Vector and Document Vector are separately stored. We have to add **.docvecs** after model name to extract Document Vector from Doc2Vec Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    print(str(model), model.docvecs.most_similar(positive=[\"Machine learning\"], topn=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, calculating cosine simillarity of **\"Lady Gaga\"** using Paragraph Vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    print(str(model), model.docvecs.most_similar(positive=[\"Lady Gaga\"], topn=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, calculating cosine simillarity of **\"Lady Gaga\" - \"American\" + \"Japanese\"** using Document vector and Word Vectors. \"American\" and \"Japanese\" are Word Vectors, not Paragraph Vectors. Word Vectors are already converted to lowercases by WikiCorpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    print(str(model), model.docvecs.most_similar([model.docvecs[\"Lady Gaga\"] - model[\"american\"] + model[\"japanese\"]], topn=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, DBOW reveal "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
