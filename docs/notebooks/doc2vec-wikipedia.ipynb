{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec to wikipedia articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We conduct the similar experiment to **Document Embedding with Paragraph Vectors** (http://arxiv.org/abs/1507.07998).\n",
    "In this paper, they showed only DBOW results to Wikipedia data. So we replicate this experiments using not only DBOW but also DM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import Doc2Vec module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.corpora.wikicorpus import WikiCorpus\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from pprint import pprint\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, download the dump of all Wikipedia articles from [here](http://download.wikimedia.org/enwiki/) (you want the file enwiki-latest-pages-articles.xml.bz2, or enwiki-YYYYMMDD-pages-articles.xml.bz2 for date-specific dumps).\n",
    "\n",
    "Second, convert the articles to WikiCorpus. WikiCorpus construct a corpus from a Wikipedia (or other MediaWiki-based) database dump.\n",
    "\n",
    "For more details on WikiCorpus, you should access [Corpus from a Wikipedia dump](https://radimrehurek.com/gensim/corpora/wikicorpus.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wiki = WikiCorpus(\"enwiki-latest-pages-articles.xml.bz2\")\n",
    "#wiki = WikiCorpus(\"enwiki-YYYYMMDD-pages-articles.xml.bz2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define **TaggedWikiDocument** class to convert WikiCorpus into suitable form for Doc2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TaggedWikiDocument(object):\n",
    "    def __init__(self, wiki):\n",
    "        self.wiki = wiki\n",
    "        self.wiki.metadata = True\n",
    "    def __iter__(self):\n",
    "        for content, (page_id, title) in self.wiki.get_texts():\n",
    "            yield TaggedDocument([c.decode(\"utf-8\") for c in content], [title])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = TaggedWikiDocument(wiki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Doc2Vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, we define some types of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dbow+w,d200,hs,w5,mc5,t8)\n",
      "Doc2Vec(dm/m,d200,hs,w5,mc5,t8)\n"
     ]
    }
   ],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "vocab =  915715 # along with paper\n",
    "\n",
    "models = [\n",
    "    # PV-DBOW \n",
    "    Doc2Vec(dm=0, dbow_words=1, size=200, window=5, min_count=5, max_vocab_size=vocab, iter=10, workers=cores),\n",
    "    # PV-DM w/average\n",
    "    Doc2Vec(dm=1, dm_mean=1, size=200, window=5, min_count=5, max_vocab_size=vocab, workers=cores),\n",
    "]\n",
    "models[0].build_vocab(documents)\n",
    "print(str(models[0]))\n",
    "models[1].reset_from(models[0])\n",
    "print(str(models[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we’re ready to train Doc2Vec of the English Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3d 18h 14min 32s, sys: 29min 16s, total: 3d 18h 43min 48s\n",
      "Wall time: 21h 48min 45s\n",
      "CPU times: user 11h 59min 25s, sys: 16min 43s, total: 12h 16min 9s\n",
      "Wall time: 7h 43min 6s\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    %time model.train(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, let's test both models! **DBOW** model show the simillar results with the original paper.\n",
    "First, calculating cosine simillarity of **\"Machine learning\"** using Paragraph Vector. Word Vector and Document Vector are separately stored. We have to add **.docvecs** after model name to extract Document Vector from Doc2Vec Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dbow+w,d200,hs,w5,mc5,t8)\n",
      "[('Artificial neural network', 0.7255396246910095),\n",
      " ('Theoretical computer science', 0.7125768661499023),\n",
      " ('Data mining', 0.6895816326141357),\n",
      " ('Pattern recognition', 0.678891658782959),\n",
      " ('List of important publications in computer science', 0.6695302724838257),\n",
      " ('Outline of computer science', 0.667578935623169),\n",
      " ('Information visualization', 0.6667760014533997),\n",
      " ('Unsupervised learning', 0.6627277135848999),\n",
      " ('Bayesian network', 0.6622973680496216),\n",
      " ('Support vector machine', 0.6594343781471252),\n",
      " ('Algorithmic composition', 0.6593101024627686),\n",
      " (\"Solomonoff's theory of inductive inference\", 0.6554585695266724),\n",
      " ('Kriging', 0.6505937576293945),\n",
      " ('Model checking', 0.6501827239990234),\n",
      " ('Information theory', 0.6447420120239258),\n",
      " ('Computational learning theory', 0.6422973871231079),\n",
      " ('Generalization error', 0.6414266228675842),\n",
      " ('Complexity', 0.6391021609306335),\n",
      " ('Glossary of artificial intelligence', 0.6353012323379517),\n",
      " ('Theory of computation', 0.6329255104064941)]\n",
      "Doc2Vec(dm/m,d200,hs,w5,mc5,t8)\n",
      "[('Theoretical computer science', 0.6481858491897583),\n",
      " ('Unsupervised learning', 0.6421648859977722),\n",
      " ('Artificial neural network', 0.637227475643158),\n",
      " ('Data stream mining', 0.6325934529304504),\n",
      " ('Pattern recognition', 0.6199989318847656),\n",
      " ('Outline of computer science', 0.6175510287284851),\n",
      " ('Deep learning', 0.6125383377075195),\n",
      " ('Algorithmic learning theory', 0.608863353729248),\n",
      " ('Statistical learning theory', 0.5993553400039673),\n",
      " ('Feature learning', 0.5990031957626343),\n",
      " ('Generalization error', 0.5920165777206421),\n",
      " ('Cognitive architecture', 0.5919679403305054),\n",
      " ('Reinforcement learning', 0.5869054794311523),\n",
      " ('Supervised learning', 0.5860797166824341),\n",
      " ('Cluster analysis', 0.5855995416641235),\n",
      " ('Data mining', 0.5830211639404297),\n",
      " ('Complexity', 0.5810916423797607),\n",
      " ('Decision tree', 0.5741485953330994),\n",
      " ('Kriging', 0.5734224915504456),\n",
      " ('Statistical inference', 0.5728234052658081)]\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print(str(model))\n",
    "    pprint(model.docvecs.most_similar(positive=[\"Machine learning\"], topn=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DBOW** model interpret the word 'Machine Learning' as a part of Computer Science field, and **DM** model as Data Science related field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, calculating cosine simillarity of **\"Lady Gaga\"** using Paragraph Vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dbow+w,d200,hs,w5,mc5,t8)\n",
      "[('Katy Perry', 0.7122470140457153),\n",
      " ('Nicki Minaj', 0.6723202466964722),\n",
      " ('Christina Aguilera', 0.6566343903541565),\n",
      " ('Adam Lambert', 0.6328957676887512),\n",
      " ('Beyoncé', 0.6275202035903931),\n",
      " ('Rihanna', 0.6249787211418152),\n",
      " ('Miley Cyrus', 0.6206135153770447),\n",
      " ('Nicole Scherzinger', 0.6199545860290527),\n",
      " ('List of awards and nominations received by Lady Gaga', 0.6158047914505005),\n",
      " ('Ariana Grande', 0.6092617511749268)]\n",
      "Doc2Vec(dm/m,d200,hs,w5,mc5,t8)\n",
      "[('Katy Perry', 0.6092739105224609),\n",
      " ('List of awards and nominations received by Lady Gaga', 0.581851065158844),\n",
      " ('Born This Way (song)', 0.5763623118400574),\n",
      " ('ArtRave: The Artpop Ball', 0.5581628084182739),\n",
      " ('The Monster Ball Tour', 0.5547659993171692),\n",
      " ('Born This Way Ball', 0.5475640892982483),\n",
      " ('Applause (Lady Gaga song)', 0.5422967672348022),\n",
      " ('The Fame', 0.5405881404876709),\n",
      " ('Rihanna', 0.5365649461746216),\n",
      " ('Paparazzi (Lady Gaga song)', 0.5345339775085449)]\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print(str(model))\n",
    "    pprint(model.docvecs.most_similar(positive=[\"Lady Gaga\"], topn=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DBOW** model reveal the similar singer in the U.S., and DM model understand that many of Lady Gaga's songs are similar with the word **\"Lady Gaga\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, calculating cosine simillarity of **\"Lady Gaga\" - \"American\" + \"Japanese\"** using Document vector and Word Vectors. \"American\" and \"Japanese\" are Word Vectors, not Paragraph Vectors. Word Vectors are already converted to lowercases by WikiCorpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dbow+w,d200,hs,w5,mc5,t8)\n",
      "[('Nicki Minaj', 0.5384640097618103),\n",
      " ('AKB48', 0.5191181302070618),\n",
      " ('Katy Perry', 0.5179253816604614),\n",
      " ('Gwen Stefani', 0.48544663190841675),\n",
      " ('Ayumi Hamasaki', 0.4849669635295868),\n",
      " ('Mondai Girl', 0.4826279282569885),\n",
      " ('Koda Kumi', 0.4788415729999542),\n",
      " ('Momoiro Clover Z', 0.47876042127609253),\n",
      " ('Big Bang (South Korean band)', 0.47655385732650757),\n",
      " ('Kyary Pamyu Pamyu', 0.4751134514808655)]\n",
      "Doc2Vec(dm/m,d200,hs,w5,mc5,t8)\n",
      "[('Aura (song)', 0.4737752676010132),\n",
      " ('Katy Perry', 0.47200503945350647),\n",
      " ('Haus of Gaga', 0.448883056640625),\n",
      " ('Born This Way (album)', 0.44316208362579346),\n",
      " ('The Fame', 0.4316578209400177),\n",
      " ('Born This Way (song)', 0.4304528832435608),\n",
      " ('Artpop', 0.42924416065216064),\n",
      " ('Marry the Night', 0.4230160117149353),\n",
      " ('List of awards and nominations received by Lady Gaga', 0.420695424079895),\n",
      " ('Speechless (Lady Gaga song)', 0.4165343940258026)]\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print(str(model))\n",
    "    vec = [model.docvecs[\"Lady Gaga\"] - model[\"american\"] + model[\"japanese\"]]\n",
    "    pprint([m for m in model.docvecs.most_similar(vec, topn=11) if m[0] != \"Lady Gaga\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, **DBOW** model demonstrate the similar artists with Lady Gaga in Japan such as 'AKB48', which is the Most famous Idol in Japan, 'Kyary Pamyu Pamyu' whose appearance is also characteristic.\n",
    "On the other hand, **DM** model results don't include the Japanese aritsts in top 10 simillar documents. It's almost same with no vector calculated results.\n",
    "\n",
    "This results demonstrate that **DBOW** employed in the original paper is outstanding for calculating the similarity between Document Vector and Word Vector."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
